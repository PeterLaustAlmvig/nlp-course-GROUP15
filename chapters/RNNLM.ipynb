{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks Tutorial, Part 2 â€“ Implementing a Language Model RNN with Python, Numpy and Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002915451895043732\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append(\"..\")\n",
    "from statnlpbook.utils import *\n",
    "import statnlpbook.util as util\n",
    "util.execute_notebook('language_models.ipynb')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\xnz224\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This the second part of the Recurrent Neural Network Tutorial. [The first part is here](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/).\n",
    "\n",
    "In this part we will implement a full Recurrent Neural Network from scratch using Python and optimize our implementation using [Theano](http://deeplearning.net/software/theano/), a library to perform operations on a GPU. **[The full code is available on Github](https://github.com/dennybritz/rnn-tutorial-rnnlm/)**. I will skip over some boilerplate code that is not essential to understanding Recurrent Neural Networks, but all of that is also [on Github](https://github.com/dennybritz/rnn-tutorial-rnnlm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling\n",
    "\n",
    "Our goal is to build a [Language Model](https://en.wikipedia.org/wiki/Language_model) using a Recurrent Neural Network. Here's what that means. Let's say we have sentence  of $m$ words. Language Model allows us to predict the probability of observing the sentence (in a given dataset) as:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "P(w_1,...,w_m) = \\prod_{i=1}^{m}P(w_i \\mid w_1,..., w_{i-1}) \n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "In words, the probability of a sentence is the product of probabilities of each word given the words that came before it. So, the probability of the sentence \"He went to buy some chocolate\" would be the probability of \"chocolate\" given \"He went to buy some\", multiplied by the probability of \"some\" given \"He went to buy\", and so on. \n",
    "\n",
    "Why is that useful? Why would we want to assign a probability to observing a sentence?\n",
    "\n",
    "First, such a model can be used as a scoring mechanism. For example, a Machine Translation system typically generates multiple candidates for an input sentence. You could use a language model to pick the most probable sentence. Intuitively, the most probable sentence is likely to be grammatically correct. Similar scoring happens in speech recognition systems.\n",
    "\n",
    "But solving the Language Modeling problem also has a cool side effect. Because we can predict the probability of a word given the preceding words, we are able to generate new text. It's a *generative model*. Given an existing sequence of words we sample a next word from the predicted probabilities, and repeat the process until we have a full sentence. Andrew Karparthy [has a great post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) that demonstrates what language models are capable of. His models are trained on single characters as opposed to full words, and can generate anything from Shakespeare to Linux Code.\n",
    "\n",
    "Note that in the above equation the probability of each word is conditioned on **all** previous words. In practice, many models have a hard time representing such long-term dependencies due to computational or memory constraints. They are typically limited to looking at only a few of the previous words. RNNs can, in theory, capture such long-term dependencies, but in practice it's a bit more complex. We'll explore that in a later post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data and Preprocessing\n",
    "\n",
    "To train our language model we need text to learn from. Fortunately we don't need any labels to train a language model, just raw text. I downloaded 15,000 longish reddit comments from a [dataset available on Google's BigQuery](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08). Text generated by our model will sound like reddit commenters (hopefully)! But as with most Machine Learning projects we first need to do some pre-processing to get our data into the right format.\n",
    "\n",
    "#### 1. Tokenize Text\n",
    "\n",
    "We have raw text, but we want to make predictions on a per-word basis. This means we must *tokenize* our comments into sentences, and sentences into words. We could just split each of the comments by spaces, but that wouldn't handle punctuation properly. The sentence \"He left!\" should be 3 tokens: \"He\", \"left\", \"!\". We'll use [NLTK's](http://www.nltk.org/) `word_tokenize` and `sent_tokenize` methods, which do most of the hard work for us.\n",
    "\n",
    "#### 2. Remove infrequent words\n",
    "\n",
    "Most words in our text will only appear one or two times. It's a good idea to remove these infrequent words. Having a huge vocabulary will make our model slow to train (we'll talk about why that is later), and because we don't have a lot of contextual examples for such words we wouldn't be able to learn how to use them correctly anyway. That's quite similar to how humans learn. To really understand how to appropriately use a word you need to have seen it in different contexts.\n",
    "\n",
    "In our code we limit our vocabulary to the `vocabulary_size` most common words (which I set to 8000, but feel free to change it). We replace all words not included in our vocabulary by `UNKNOWN_TOKEN`. For example, if we don't include the word \"nonlinearities\" in our vocabulary, the sentence \"nonlineraties are important in neural networks\" becomes \"UNKNOWN_TOKEN are important in Neural Networks\". The word `UNKNOWN_TOKEN` will become part of our vocabulary and we will predict it just like any other word. When we generate new text we can replace `UNKNOWN_TOKEN` again, for example by taking a randomly sampled word not in our vocabulary, or we could just generate sentences until we get one that doesn't contain an unknown token.\n",
    "\n",
    "#### 3. Prepend special start and end tokens\n",
    "\n",
    "We also want to learn which words tend start and end a sentence. To do this we prepend a special `SENTENCE_START` token, and append a special `SENTENCE_END` token to each sentence. This allows us to ask: Given that the first token is `SENTENCE_START`, what is the likely next word (the actual first word of the sentence)?\n",
    "\n",
    "\n",
    "#### 4. Build training data matrices\n",
    "\n",
    "The input to our Recurrent Neural Networks are vectors, not strings. So we create a mapping between words and indices, `index_to_word`, and `word_to_index`. For example,  the word \"friendly\" may be at index 2001. A training example $x$ may look like `[0, 179, 341, 416]`, where 0 corresponds to `SENTENCE_START`. The corresponding label $y$ would be `[179, 341, 416, 1]`. Remember that our goal is to predict the next word, so y is just the x vector shifted by one position with the last element being the `SENTENCE_END` token. In other words, the correct prediction for word `179` above would be `341`, the actual next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 212 sentences.\n",
      "Found 3470 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'complaint' and appeared 1 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START [BAR]Can't even call this a blues song[/BAR][BAR]It's been so long[/BAR][BAR]Neither one of us was wrong or anything like that[/BAR][BAR]It seems like yesterday[/BAR][BAR][/BAR][BAR][Chorus][/BAR][BAR]In the silence of the city night[/BAR][BAR]When the lonely watch the sky in yearning[/BAR][BAR]I, at rest, lie in peace beside you[/BAR][BAR]I searched a thousand skies before you came[/BAR][BAR][/BAR][BAR][Verse 1][/BAR][BAR]For the 4th 3rd, I Chris Columbus three words from Stevie for the issue[/BAR][BAR]The only thing to call it was official[/BAR][BAR]The way I live for you was as if I die with you[/BAR][BAR]Cause not a moment I spent with you was artifical[/BAR][BAR]The plan from the dome was to build a home with you[/BAR][BAR]Forever had a space in my heart, I roam with you[/BAR][BAR]Hypothetical dreams conjure when I met you[/BAR][BAR]See, years grew between us and stil I can't forget you[/BAR][BAR]Regretting how I ever let you let me let you escape fools paradise[/BAR][BAR]Running from a paradox[/BAR][BAR]Living like a pair of ducks, but with different flocks[/BAR][BAR]With different destinations, our ships remain docked[/BAR][BAR]Temporary, but the feelings that I harbor stil wade[/BAR][BAR]In the holy waters that made our sweet lemonaid[/BAR][BAR]Too dumb to persist, too smart to persuade[/BAR][BAR]Too heavy to push aside and too stong to stay[/BAR][BAR]Too hard to work and so easy to play[/BAR][BAR]Like chopsticks, what I picked slipped away[/BAR][BAR]Leaving me to recollect day by day[/BAR][BAR]And take solace in the words Bill say[/BAR][BAR][/BAR][BAR][Chorus][/BAR][BAR]Memories take you back, to the good times[/BAR][BAR]When it's over and sad times disappear[/BAR][BAR]Memories take you back[/BAR][BAR]To the lean times, in between times[/BAR][BAR]To the days of yesteryear[/BAR][BAR]Memories are that way[/BAR][BAR][/BAR][BAR][Verse 2][/BAR][BAR]Memories are that way, that way back to that way back[/BAR][BAR]When I could never think of you and say \"wack\"[/BAR][BAR]Similar to old cassettes, something to cuss at[/BAR][BAR]Deteriorating with each rewind and playback[/BAR][BAR]Even now, one of the hits I remenice[/BAR][BAR]On this angelic face, reciplicating bliss[/BAR][BAR]With the radio low, your feet anchored in mine[/BAR][BAR]Hands to waistline, walking to a bassline[/BAR][BAR]Good food in my belly, good love on my mind[/BAR][BAR]Turn around to a kiss deep enough to stop time[/BAR][BAR]Open up to a smile so bright, I go blind[/BAR][BAR]To the suffering of the world, everything appears fine[/BAR][BAR]Poetry in the bed, literally[/BAR][BAR]You putting me open to Stephanie and Giovanni[/BAR][BAR]With legs entwined, trading profound lines[/BAR][BAR]Fingers and spines aligned, digging in your mind like it's mine[/BAR][BAR]Sometimes at night it seem so damn right[/BAR][BAR]Just to shift nose to neck, just to get a whiff[/BAR][BAR]I've never seen life like this, so life-like[/BAR][BAR]This is how I'd like my life to subsist[/BAR][BAR]But somewhere in between us were interpetations[/BAR][BAR]Of justice and Jesus, cultures in said nations[/BAR][BAR]But if I woulda knew what I know now, mighta never known how[/BAR][BAR][/BAR][BAR][Chorus][/BAR][BAR]Memories take you back, to the good times[/BAR][BAR]When it's over[/BAR][BAR]I searched a thousand skies before you came[/BAR][BAR]And in the morning, when the world is new[/BAR][BAR]The lonely turn away, as I turn to you, beside me[/BAR][BAR][/BAR][BAR][Verse 3][/BAR][BAR]The greatest story ever erased and never replaced[/BAR][BAR]The worst torture ever I faced[/BAR][BAR]Was trying to retrace the steps in my mind[/BAR][BAR]Like a defeated surgeon fighting fate with the cure[/BAR][BAR]Just a little too late[/BAR][BAR]We went from \"Yo, who's that?\" SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', '[', 'BAR', ']', 'Ca', \"n't\", 'even', 'call', 'this', 'a', 'blues', 'song', '[', '/BAR', ']', '[', 'BAR', ']', 'It', \"'s\", 'been', 'so', 'long', '[', '/BAR', ']', '[', 'BAR', ']', 'Neither', 'one', 'of', 'us', 'was', 'wrong', 'or', 'anything', 'like', 'that', '[', '/BAR', ']', '[', 'BAR', ']', 'It', 'seems', 'like', 'yesterday', '[', '/BAR', ']', '[', 'BAR', ']', '[', '/BAR', ']', '[', 'BAR', ']', '[', 'Chorus', ']', '[', '/BAR', ']', '[', 'BAR', ']', 'In', 'the', 'silence', 'of', 'the', 'city', 'night', '[', '/BAR', ']', '[', 'BAR', ']', 'When', 'the', 'lonely', 'watch', 'the', 'sky', 'in', 'yearning', '[', '/BAR', ']', '[', 'BAR', ']', 'I', ',', 'at', 'rest', ',', 'lie', 'in', 'peace', 'beside', 'you', '[', '/BAR', ']', '[', 'BAR', ']', 'I', 'searched', 'a', 'thousand', 'skies', 'before', 'you', 'came', '[', '/BAR', ']', '[', 'BAR', ']', '[', '/BAR', ']', '[', 'BAR', ']', '[', 'Verse', '1', ']', '[', '/BAR', ']', '[', 'BAR', ']', 'For', 'the', '4th', '3rd', ',', 'I', 'Chris', 'Columbus', 'three', 'words', 'from', 'Stevie', 'for', 'the', 'issue', '[', '/BAR', ']', '[', 'BAR', ']', 'The', 'only', 'thing', 'to', 'call', 'it', 'was', 'official', '[', '/BAR', ']', '[', 'BAR', ']', 'The', 'way', 'I', 'live', 'for', 'you', 'was', 'as', 'if', 'I', 'die', 'with', 'you', '[', '/BAR', ']', '[', 'BAR', ']', 'Cause', 'not', 'a', 'moment', 'I', 'spent', 'with', 'you', 'was', 'artifical', '[', '/BAR', ']', '[', 'BAR', ']', 'The', 'plan', 'from', 'the', 'dome', 'was', 'to', 'build', 'a', 'home', 'with', 'you', '[', '/BAR', ']', '[', 'BAR', ']', 'Forever', 'had', 'a', 'space', 'in', 'my', 'heart', ',', 'I', 'roam', 'with', 'you', '[', '/BAR', ']', '[', 'BAR', ']', 'Hypothetical', 'dreams', 'conjure', 'when', 'I', 'met', 'you', '[', '/BAR', ']', '[', 'BAR', ']', 'See', ',', 'years', 'grew', 'between', 'us', 'and', 'stil', 'I', 'ca', \"n't\", 'forget', 'you', '[', '/BAR', ']', '[', 'BAR', ']', 'Regretting', 'how', 'I', 'ever', 'let', 'you', 'let', 'me', 'let', 'you', 'escape', 'fools', 'paradise', '[', '/BAR', ']', '[', 'BAR', ']', 'Running', 'from', 'a', 'paradox', '[', '/BAR', ']', '[', 'BAR', ']', 'Living', 'like', 'a', 'pair', 'of', 'ducks', ',', 'but', 'with', 'different', 'flocks', '[', '/BAR', ']', '[', 'BAR', ']', 'With', 'different', 'destinations', ',', 'our', 'ships', 'remain', 'docked', '[', '/BAR', ']', '[', 'BAR', ']', 'Temporary', ',', 'but', 'the', 'feelings', 'that', 'I', 'harbor', 'stil', 'wade', '[', '/BAR', ']', '[', 'BAR', ']', 'In', 'the', 'holy', 'waters', 'that', 'made', 'our', 'sweet', 'lemonaid', '[', '/BAR', ']', '[', 'BAR', ']', 'Too', 'dumb', 'to', 'persist', ',', 'too', 'smart', 'to', 'persuade', '[', '/BAR', ']', '[', 'BAR', ']', 'Too', 'heavy', 'to', 'push', 'aside', 'and', 'too', 'stong', 'to', 'stay', '[', '/BAR', ']', '[', 'BAR', ']', 'Too', 'hard', 'to', 'work', 'and', 'so', 'easy', 'to', 'play', '[', '/BAR', ']', '[', 'BAR', ']', 'Like', 'chopsticks', ',', 'what', 'I', 'picked', 'slipped', 'away', '[', '/BAR', ']', '[', 'BAR', ']', 'Leaving', 'me', 'to', 'recollect', 'day', 'by', 'day', '[', '/BAR', ']', '[', 'BAR', ']', 'And', 'take', 'solace', 'in', 'the', 'words', 'Bill', 'say', '[', '/BAR', ']', '[', 'BAR', ']', '[', '/BAR', ']', '[', 'BAR', ']', '[', 'Chorus', ']', '[', '/BAR', ']', '[', 'BAR', ']', 'Memories', 'take', 'you', 'back', ',', 'to', 'the', 'good', 'times', '[', '/BAR', ']', '[', 'BAR', ']', 'When', 'it', \"'s\", 'over', 'and', 'sad', 'times', 'disappear', '[', '/BAR', ']', '[', 'BAR', ']', 'Memories', 'take', 'you', 'back', '[', '/BAR', ']', '[', 'BAR', ']', 'To', 'the', 'lean', 'times', ',', 'in', 'between', 'times', '[', '/BAR', ']', '[', 'BAR', ']', 'To', 'the', 'days', 'of', 'yesteryear', '[', '/BAR', ']', '[', 'BAR', ']', 'Memories', 'are', 'that', 'way', '[', '/BAR', ']', '[', 'BAR', ']', '[', '/BAR', ']', '[', 'BAR', ']', '[', 'Verse', '2', ']', '[', '/BAR', ']', '[', 'BAR', ']', 'Memories', 'are', 'that', 'way', ',', 'that', 'way', 'back', 'to', 'that', 'way', 'back', '[', '/BAR', ']', '[', 'BAR', ']', 'When', 'I', 'could', 'never', 'think', 'of', 'you', 'and', 'say', '``', 'wack', \"''\", '[', '/BAR', ']', '[', 'BAR', ']', 'Similar', 'to', 'old', 'cassettes', ',', 'something', 'to', 'cuss', 'at', '[', '/BAR', ']', '[', 'BAR', ']', 'Deteriorating', 'with', 'each', 'rewind', 'and', 'playback', '[', '/BAR', ']', '[', 'BAR', ']', 'Even', 'now', ',', 'one', 'of', 'the', 'hits', 'I', 'remenice', '[', '/BAR', ']', '[', 'BAR', ']', 'On', 'this', 'angelic', 'face', ',', 'reciplicating', 'bliss', '[', '/BAR', ']', '[', 'BAR', ']', 'With', 'the', 'radio', 'low', ',', 'your', 'feet', 'anchored', 'in', 'mine', '[', '/BAR', ']', '[', 'BAR', ']', 'Hands', 'to', 'waistline', ',', 'walking', 'to', 'a', 'bassline', '[', '/BAR', ']', '[', 'BAR', ']', 'Good', 'food', 'in', 'my', 'belly', ',', 'good', 'love', 'on', 'my', 'mind', '[', '/BAR', ']', '[', 'BAR', ']', 'Turn', 'around', 'to', 'a', 'kiss', 'deep', 'enough', 'to', 'stop', 'time', '[', '/BAR', ']', '[', 'BAR', ']', 'Open', 'up', 'to', 'a', 'smile', 'so', 'bright', ',', 'I', 'go', 'blind', '[', '/BAR', ']', '[', 'BAR', ']', 'To', 'the', 'suffering', 'of', 'the', 'world', ',', 'everything', 'appears', 'fine', '[', '/BAR', ']', '[', 'BAR', ']', 'Poetry', 'in', 'the', 'bed', ',', 'literally', '[', '/BAR', ']', '[', 'BAR', ']', 'You', 'putting', 'me', 'open', 'to', 'Stephanie', 'and', 'Giovanni', '[', '/BAR', ']', '[', 'BAR', ']', 'With', 'legs', 'entwined', ',', 'trading', 'profound', 'lines', '[', '/BAR', ']', '[', 'BAR', ']', 'Fingers', 'and', 'spines', 'aligned', ',', 'digging', 'in', 'your', 'mind', 'like', 'it', \"'s\", 'mine', '[', '/BAR', ']', '[', 'BAR', ']', 'Sometimes', 'at', 'night', 'it', 'seem', 'so', 'damn', 'right', '[', '/BAR', ']', '[', 'BAR', ']', 'Just', 'to', 'shift', 'nose', 'to', 'neck', ',', 'just', 'to', 'get', 'a', 'whiff', '[', '/BAR', ']', '[', 'BAR', ']', 'I', \"'ve\", 'never', 'seen', 'life', 'like', 'this', ',', 'so', 'life-like', '[', '/BAR', ']', '[', 'BAR', ']', 'This', 'is', 'how', 'I', \"'d\", 'like', 'my', 'life', 'to', 'subsist', '[', '/BAR', ']', '[', 'BAR', ']', 'But', 'somewhere', 'in', 'between', 'us', 'were', 'interpetations', '[', '/BAR', ']', '[', 'BAR', ']', 'Of', 'justice', 'and', 'Jesus', ',', 'cultures', 'in', 'said', 'nations', '[', '/BAR', ']', '[', 'BAR', ']', 'But', 'if', 'I', 'woulda', 'knew', 'what', 'I', 'know', 'now', ',', 'mighta', 'never', 'known', 'how', '[', '/BAR', ']', '[', 'BAR', ']', '[', '/BAR', ']', '[', 'BAR', ']', '[', 'Chorus', ']', '[', '/BAR', ']', '[', 'BAR', ']', 'Memories', 'take', 'you', 'back', ',', 'to', 'the', 'good', 'times', '[', '/BAR', ']', '[', 'BAR', ']', 'When', 'it', \"'s\", 'over', '[', '/BAR', ']', '[', 'BAR', ']', 'I', 'searched', 'a', 'thousand', 'skies', 'before', 'you', 'came', '[', '/BAR', ']', '[', 'BAR', ']', 'And', 'in', 'the', 'morning', ',', 'when', 'the', 'world', 'is', 'new', '[', '/BAR', ']', '[', 'BAR', ']', 'The', 'lonely', 'turn', 'away', ',', 'as', 'I', 'turn', 'to', 'you', ',', 'beside', 'me', '[', '/BAR', ']', '[', 'BAR', ']', '[', '/BAR', ']', '[', 'BAR', ']', '[', 'Verse', '3', ']', '[', '/BAR', ']', '[', 'BAR', ']', 'The', 'greatest', 'story', 'ever', 'erased', 'and', 'never', 'replaced', '[', '/BAR', ']', '[', 'BAR', ']', 'The', 'worst', 'torture', 'ever', 'I', 'faced', '[', '/BAR', ']', '[', 'BAR', ']', 'Was', 'trying', 'to', 'retrace', 'the', 'steps', 'in', 'my', 'mind', '[', '/BAR', ']', '[', 'BAR', ']', 'Like', 'a', 'defeated', 'surgeon', 'fighting', 'fate', 'with', 'the', 'cure', '[', '/BAR', ']', '[', 'BAR', ']', 'Just', 'a', 'little', 'too', 'late', '[', '/BAR', ']', '[', 'BAR', ']', 'We', 'went', 'from', '``', 'Yo', ',', 'who', \"'s\", 'that', '?', \"''\", 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "sentences = itertools.chain(*[nltk.sent_tokenize(x) for x in trainDocs])\n",
    "# Append SENTENCE_START and SENTENCE_END\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print( \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print( \"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print( \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences], dtype=\"object\")\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences], dtype=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an actual training example from our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START that I 'm kicking while I 'm walking the street [ /BAR ] [ BAR ] I write rhymes just so I can go ( clear throat ) on beat [ /BAR ] [ BAR ] You thinks its so damn luvly do n't you nigga come here [ /BAR ] [ BAR ] You just a frightened little kid all steps and no beard [ /BAR ] [ BAR ] But let me tell you now your talking to a grown ass man [ /BAR ] [ BAR ] I put some shit in your head to fill your stomach to head [ /BAR ] [ BAR ] You do n't believe me well it 's taken me from here to Japan [ /BAR ] [ BAR ] That 's why I 'm laughing to the bank without a getaway van [ /BAR ] [ BAR ] You worrying about your ice and want your shorty to see [ /BAR ] [ BAR ] The only ice that I need is in my L I T [ /BAR ] [ BAR ] I keep my Hennessey straight [ /BAR ] [ BAR ] So skip the fantasy plan [ /BAR ] [ BAR ] That 's why you get up and go [ /BAR ] [ BAR ] That 's shit you got to debate [ /BAR ] [ BAR ] You steady worrying bout your cry-stal [ /BAR ] [ BAR ] Charge your pis-tol [ /BAR ] [ BAR ] Meanwhile [ /BAR ] [ BAR ] The real style [ /BAR ] [ BAR ] ( Long gone ?\n",
      "[13, 15, 6, 34, 1671, 380, 6, 34, 851, 5, 633, 0, 3, 1, 0, 2, 1, 6, 301, 186, 39, 73, 6, 71, 128, 22, 1672, 1673, 23, 27, 184, 0, 3, 1, 0, 2, 1, 38, 1674, 245, 73, 211, 1675, 29, 16, 7, 536, 151, 150, 0, 3, 1, 0, 2, 1, 38, 39, 9, 1676, 140, 218, 57, 497, 12, 90, 1677, 0, 3, 1, 0, 2, 1, 46, 107, 36, 219, 7, 70, 20, 335, 8, 9, 674, 164, 63, 0, 3, 1, 0, 2, 1, 6, 250, 101, 40, 17, 20, 336, 8, 961, 20, 962, 8, 336, 0, 3, 1, 0, 2, 1, 38, 29, 16, 675, 36, 337, 10, 11, 955, 36, 48, 150, 8, 1678, 0, 3, 1, 0, 2, 1, 98, 11, 123, 6, 34, 963, 8, 5, 1679, 451, 9, 1680, 1681, 0, 3, 1, 0, 2, 1, 38, 964, 105, 20, 676, 12, 165, 20, 965, 8, 78, 0, 3, 1, 0, 2, 1, 41, 192, 676, 15, 6, 166, 31, 17, 25, 966, 6, 967, 0, 3, 1, 0, 2, 1, 6, 381, 25, 1682, 243, 0, 3, 1, 0, 2, 1, 58, 537, 5, 1683, 350, 0, 3, 1, 0, 2, 1, 98, 11, 123, 7, 42, 51, 12, 128, 0, 3, 1, 0, 2, 1, 98, 11, 40, 7, 45, 8, 538, 0, 3, 1, 0, 2, 1, 38, 1684, 964, 338, 20, 1685, 0, 3, 1, 0, 2, 1, 1686, 20, 1687, 0, 3, 1, 0, 2, 1, 968, 0, 3, 1, 0, 2, 1, 41, 124, 125, 0, 3, 1, 0, 2, 1, 22, 969, 251, 26]\n",
      "\n",
      "y:\n",
      "that I 'm kicking while I 'm walking the street [ /BAR ] [ BAR ] I write rhymes just so I can go ( clear throat ) on beat [ /BAR ] [ BAR ] You thinks its so damn luvly do n't you nigga come here [ /BAR ] [ BAR ] You just a frightened little kid all steps and no beard [ /BAR ] [ BAR ] But let me tell you now your talking to a grown ass man [ /BAR ] [ BAR ] I put some shit in your head to fill your stomach to head [ /BAR ] [ BAR ] You do n't believe me well it 's taken me from here to Japan [ /BAR ] [ BAR ] That 's why I 'm laughing to the bank without a getaway van [ /BAR ] [ BAR ] You worrying about your ice and want your shorty to see [ /BAR ] [ BAR ] The only ice that I need is in my L I T [ /BAR ] [ BAR ] I keep my Hennessey straight [ /BAR ] [ BAR ] So skip the fantasy plan [ /BAR ] [ BAR ] That 's why you get up and go [ /BAR ] [ BAR ] That 's shit you got to debate [ /BAR ] [ BAR ] You steady worrying bout your cry-stal [ /BAR ] [ BAR ] Charge your pis-tol [ /BAR ] [ BAR ] Meanwhile [ /BAR ] [ BAR ] The real style [ /BAR ] [ BAR ] ( Long gone ? SENTENCE_END\n",
      "[15, 6, 34, 1671, 380, 6, 34, 851, 5, 633, 0, 3, 1, 0, 2, 1, 6, 301, 186, 39, 73, 6, 71, 128, 22, 1672, 1673, 23, 27, 184, 0, 3, 1, 0, 2, 1, 38, 1674, 245, 73, 211, 1675, 29, 16, 7, 536, 151, 150, 0, 3, 1, 0, 2, 1, 38, 39, 9, 1676, 140, 218, 57, 497, 12, 90, 1677, 0, 3, 1, 0, 2, 1, 46, 107, 36, 219, 7, 70, 20, 335, 8, 9, 674, 164, 63, 0, 3, 1, 0, 2, 1, 6, 250, 101, 40, 17, 20, 336, 8, 961, 20, 962, 8, 336, 0, 3, 1, 0, 2, 1, 38, 29, 16, 675, 36, 337, 10, 11, 955, 36, 48, 150, 8, 1678, 0, 3, 1, 0, 2, 1, 98, 11, 123, 6, 34, 963, 8, 5, 1679, 451, 9, 1680, 1681, 0, 3, 1, 0, 2, 1, 38, 964, 105, 20, 676, 12, 165, 20, 965, 8, 78, 0, 3, 1, 0, 2, 1, 41, 192, 676, 15, 6, 166, 31, 17, 25, 966, 6, 967, 0, 3, 1, 0, 2, 1, 6, 381, 25, 1682, 243, 0, 3, 1, 0, 2, 1, 58, 537, 5, 1683, 350, 0, 3, 1, 0, 2, 1, 98, 11, 123, 7, 42, 51, 12, 128, 0, 3, 1, 0, 2, 1, 98, 11, 40, 7, 45, 8, 538, 0, 3, 1, 0, 2, 1, 38, 1684, 964, 338, 20, 1685, 0, 3, 1, 0, 2, 1, 1686, 20, 1687, 0, 3, 1, 0, 2, 1, 968, 0, 3, 1, 0, 2, 1, 41, 124, 125, 0, 3, 1, 0, 2, 1, 22, 969, 251, 26, 14]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print (\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the RNN\n",
    "\n",
    "For a general overview of RNNs take a look at [first part of the tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/).\n",
    "\n",
    "![](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "Let's get concrete and see what the RNN for our language model looks like. The input $x$ will be a sequence of words (just like the example printed above) and each $x_t$ is a single word. But there's one more thing: Because of how matrix multiplication works we can't simply use a word index (like 36) as an input. Instead, we represent each word as a *one-hot vector* of size `vocabulary_size`. For example, the word with index 36 would be the vector of all 0's and a 1 at position 36. So, each $x_t$ will become a vector, and $x$ will be a matrix, with each row representing a word. We'll perform this transformation in our Neural Network code instead of doing it in the pre-processing. The output of our network $o$ has a similar format. Each $o_t$ is a vector of `vocabulary_size` elements, and each element represents the probability of that word being the next word in the sentence.\n",
    "\n",
    "Let's recap the equations for the RNN from the first part of the tutorial:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "s_t &= \\tanh(Ux_t + Ws_{t-1}) \\\\\n",
    "o_t &= \\mathrm{softmax}(Vs_t)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "I always find it useful to write down the dimensions of the matrices and vectors. Let's assume we pick a vocabulary size $C = 8000$ and a hidden layer size $H = 100$. You can think of the hidden layer size as the \"memory\" of our network. Making it bigger allows us to learn more complex patterns, but also results in additional computation. Then we have:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "x_t & \\in \\mathbb{R}^{8000} \\\\\n",
    "o_t & \\in \\mathbb{R}^{8000} \\\\\n",
    "s_t & \\in \\mathbb{R}^{100} \\\\\n",
    "U & \\in \\mathbb{R}^{100 \\times 8000} \\\\\n",
    "V & \\in \\mathbb{R}^{8000 \\times 100} \\\\\n",
    "W & \\in \\mathbb{R}^{100 \\times 100} \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "This is valuable information. Remember that $U,V$ and $W$ are the parameters of our network we want to learn from data. Thus, we need to learn a total of $2HC + H^2$ parameters. In the case of $C=8000$ and $H=100$ that's 1,610,000.  The dimensions also tell us the bottleneck of our model. Note that because $x_t$ is a one-hot vector, multiplying it with $U$ is essentially the same as selecting a column of U, so we don't need to perform the full multiplication. Then, the biggest matrix multiplication in our network is $Vs_t$. That's why we want to keep our vocabulary size small if possible.\n",
    "\n",
    "Armed with this, it's time to start our implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "We start by declaring a RNN class an initializing our parameters. I'm calling this class `RNNNumpy` because we will implement a Theano version later. Initializing the parameters $U,V$ and $W$ is a bit tricky. We can't just initialize them to 0's because that would result in symmetric calculations in all our layers. We must initialize them randomly. Because proper initialization seems to have an impact on training results there has been lot of research in this area. It turns out that the best initialization depends on the activation function ($\\tanh$ in our case) and one [recommended](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) approach is to initialize the weights randomly in the interval from $\\left[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}\\right]$ where $n$ is the number of incoming connections from the previous layer. This may sound overly complicated, but don't worry too much about it. As long as you initialize your parameters to small random values it typically works out fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, `word_dim` is the size of our vocabulary, and `hidden_dim` is the size of our hidden layer (we can pick it). Don't worry about the `bptt_truncate` parameter for now, we'll explain what that is later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "Next, let's implement the forward propagation (predicting word probabilities) defined by our equations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We not only return the calculated outputs, but also the hidden states. We will use them later to calculate the gradients, and by returning them here we avoid duplicate computation. Each $o_t$ is a vector of probabilities representing the words in our vocabulary, but sometimes, for example when evaluating our model, all we want is the next word with the highest probability. We call this function `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our newly implemented methods and see an example output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 8000)\n",
      "[[0.00012409 0.00012392 0.00012493 ... 0.00012486 0.00012504 0.00012556]\n",
      " [0.00012416 0.00012455 0.00012624 ... 0.00012501 0.00012484 0.00012547]\n",
      " [0.00012522 0.00012526 0.00012429 ... 0.000125   0.00012517 0.00012522]\n",
      " ...\n",
      " [0.00012511 0.0001247  0.00012458 ... 0.00012518 0.00012409 0.0001246 ]\n",
      " [0.0001249  0.00012534 0.00012596 ... 0.00012552 0.00012533 0.0001246 ]\n",
      " [0.00012511 0.00012569 0.00012537 ... 0.00012503 0.00012544 0.000125  ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print (o.shape)\n",
    "print (o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the sentence (45 above), our model made 8000 predictions representing probabilities of the next word. Note that because we initialized $U,V,W$ to random values these predictions are completely random right now. The following gives the indices of the highest probability predictions for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36,)\n",
      "[2133 1284 3106 6510 5763 5027 2563 7316  870 7654  883 2785 5828  923\n",
      " 4552 6832 5915 6795 3106 6510 2430 5027 2563 7878  824 2794 6737 3107\n",
      " 7230 2925 6915 7499 4552 4358 3700 6559]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print (predictions.shape)\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Loss\n",
    "\n",
    "To train our network we need a way to measure the errors it makes. We call this the loss function $L$, and our goal is find the parameters $U,V$ and $W$ that minimize the loss function for our training data. A common choice for the loss function is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). If we have $N$ training examples (words in our text) and $C$ classes (the size of our vocabulary) then the loss with respect to our predictions $o$ and the true labels $y$ is given by:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "L(y,o) = - \\frac{1}{N} \\sum_{n \\in N} y_{n} \\log o_{n}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "The formula looks a bit complicated, but all it really does is sum over our training examples and add to the loss based on how off our prediction are. The further away $y$ (the correct words) and $o$ (our predictions), the greater the loss will be. We implement the function `calculate_loss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a step back and think about what the loss should be for random predictions. That will give us a baseline and make sure our implementation is correct. We have $C$ words in our vocabulary, so each word should be (on average) predicted with probability $1/C$, which would yield a loss of $L = -\\frac{1}{N} N \\log\\frac{1}{C} = \\log C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xnz224\\AppData\\Local\\Temp\\ipykernel_9456\\1460852643.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 8.987441\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print (\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print (\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close! Keep in mind that evaluating the loss on the full dataset is an expensive operation and can take hours if you have a lot of data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the RNN with SGD and Backpropagation Through Time (BPTT)\n",
    "\n",
    "Remember that we want to find the parameters $U,V$ and $W$ that minimize the total loss on the training data. The most common way to do this is SGD, Stochastic Gradient Descent. The idea behind SGD is pretty simple. We iterate over all our training examples and during each iteration we nudge the parameters into a direction that reduces the error. These directions are given by the gradients on the loss: $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$. SGD also needs a *learning rate*, which defines how big of a step we want to make in each iteration. SGD is the most popular optimization method not only for Neural Networks, but also for many other Machine Learning algorithms. As such there has been a lot of research on how to optimize SGD using batching, parallelism and adaptive learning rates. Even though the basic idea is simple, implementing SGD in a really efficient way can become very complex. If you want to learn more about SGD [this](http://cs231n.github.io/optimization-1/) is a good place to start. Due to its popularity there are a wealth of tutorials floating around the web, and I don't want to duplicate them here. I'll implement a simple version of SGD that should be understandable even without a background in optimization.\n",
    "\n",
    "But how do we calculate those gradients we mentioned above? In a [traditional Neural Network](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/) we do this through the backpropagation algorithm. In RNNs we use a slightly modified version of the this algorithm called Backpropagation Through Time (BPTT). Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. If you know calculus, it really is just applying the chain rule. The next part of the tutorial will be all about BPTT, so I won't go into detailed derivation here. For a general introduction to backpropagation check out [this](http://colah.github.io/posts/2015-08-Backprop/) and this [post](http://cs231n.github.io/optimization-2/). For now you can treat BPTT as a black box. It takes as input a training example $(x,y)$ and returns the gradients $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Checking\n",
    "\n",
    "Whenever you implement backpropagation it is good idea to also implement *gradient checking*, which is a way of verifying that your implementation is correct. The idea behind gradient checking is that derivative of a parameter is equal to the slope at the point, which we can approximate by slightly changing the parameter and then dividing by the change:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\lim_{h \\to 0} \\frac{J(\\theta + h) - J(\\theta -h)}{2h}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "We then compare the gradient we calculated using backpropagation to the gradient we estimated with the method above. If there's no large difference we are good. The approximation needs to calculate the total loss for *every* parameter, so that gradient checking is very expensive (remember, we had more than a million parameters in the example above). So it's a good idea to perform it on a model with a smaller vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xnz224\\AppData\\Local\\Temp\\ipykernel_9456\\1865409233.py:28: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print (\"+h Loss: %f\" % gradplus)\n",
    "                print (\"-h Loss: %f\" % gradminus)\n",
    "                print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print (\"Relative Error: %f\" % relative_error)\n",
    "                return \n",
    "            it.iternext()\n",
    "        print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Implementation\n",
    "\n",
    "Now that we are able to calculate the gradients for our parameters we can implement SGD. I like to do this in two steps: 1. A function `sdg_step` that calculates the gradients and performs the updates for one batch. 2. An outer loop that iterates through the training set and adjusts the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Let's try to get a sense of how long it would take to train our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms Â± 2.56 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, bad news. One step of SGD takes approximately 350 milliseconds on my laptop. We have about 80,000 examples in our training data, so one epoch (iteration over the whole data set) would take several hours. Multiple epochs would take days, or even weeks! And we're still working with a small dataset compared to what's being used by many of the companies and researchers out there. What now?\n",
    "\n",
    "Fortunately there are many ways to speed up our code. We could stick with the same model and make our code run faster, or we could modify our model to be less computationally expensive, or both. Researchers have identified many ways to make models less computationally expensive, for example by using a hierarchical softmax or adding projection layers to avoid the large matrix multiplications (see also [here](http://arxiv.org/pdf/1301.3781.pdf) or [here](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)). But I want to keep our model simple and go the first route: Make our implementation run faster using a GPU. Before doing that though, let's just try to run SGD with a small dataset and check if the loss actually decreases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xnz224\\AppData\\Local\\Temp\\ipykernel_9456\\1460852643.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-12 15:10:43: Loss after num_examples_seen=0 epoch=0: 8.987421\n",
      "2024-09-12 15:11:20: Loss after num_examples_seen=100 epoch=1: 9.905413\n",
      "Setting learning rate to 0.002500\n",
      "2024-09-12 15:11:55: Loss after num_examples_seen=200 epoch=2: 9.425783\n",
      "2024-09-12 15:12:33: Loss after num_examples_seen=300 epoch=3: 31.447634\n",
      "Setting learning rate to 0.001250\n",
      "2024-09-12 15:13:08: Loss after num_examples_seen=400 epoch=4: 7.553345\n",
      "2024-09-12 15:13:45: Loss after num_examples_seen=500 epoch=5: 12.306053\n",
      "Setting learning rate to 0.000625\n",
      "2024-09-12 15:14:22: Loss after num_examples_seen=600 epoch=6: 5.435099\n",
      "2024-09-12 15:14:59: Loss after num_examples_seen=700 epoch=7: 5.200840\n",
      "2024-09-12 15:15:36: Loss after num_examples_seen=800 epoch=8: 4.745959\n",
      "2024-09-12 15:16:13: Loss after num_examples_seen=900 epoch=9: 5.663882\n",
      "Setting learning rate to 0.000313\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it seems like our implementation is at least doing something useful and decreasing the loss, just like we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generating Text\n",
    "\n",
    "Now that we have our model we can ask it to generate new text for us! Let's implement a helper function to generate new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'' as ) mind na that It the `` is ? if '' a ca even You how ) by But it as [ Chorus SENTENCE_START that can never when what J-Live ... n't man it do that back /BAR ! 'm but can '' So So as Chorus Cause they so I { was from ca BAR see so when * mind n't na with ... say your up it ] Chorus ! ! got now so 're stroke you when 'm - * SENTENCE_START ? , see with I BAR with ' 're /BAR they and like my shit shit To SENTENCE_START at they that know do a life say stroke na how even BAR at ! /BAR as shit It when with by man even And just na for [ but You , when - see when To no Cause n't Cause * That a ' ca ( make { [ 're when Chorus ) ] SENTENCE_START To know with when now ? through BAR how they my - do mind up your to out , Anna Cause `` to got ' I mind but one and stroke when out my know for one Chorus 'm what with [ now BAR - as even of `` so ] It or J-Live me time ( say now - ca back got they through { my up can back just even the out ) one as '' the for 's So for just To SENTENCE_START I ! was and on when `` like and I ? '' got `` do just all ) J-Live BAR got n't one they to ca on ca It the see That } ... SENTENCE_START you\n",
      "or ... a * from you see he ! your that - they was one see It The for through it with one no know stroke 's know from one SENTENCE_START 're by no 're can - and and mind I - from [ how now do your na if Chorus ] when Anna what they the but for for me up he - `` how now n't ' It how do what mind back from get was Anna if It me good this a if if SENTENCE_START to make if for like Anna not one stroke as ) back BAR do if ai Anna or do stroke that all but back ] and we at this { } ( me for they no but ! The make all I The one on ai 's all when be when Cause good and * was a { through } as do just life man a one that You be is one 're stroke so SENTENCE_START ai ! Chorus that like in be time at - out got good your he for BAR Chorus '' But ``\n",
      "that stroke but the man life that up 'm make ... SENTENCE_START It make in J-Live It on even out one good /BAR all ... and that But with when never even `` mind a back me or J-Live shit mind can through ca got know n't was That like stroke na they make ) the\n",
      "'re - through ai not To time 'm but Anna ) SENTENCE_START [ see BAR do /BAR time man man ) The and it 'm with the back * make from no but ai how You BAR for up The To they by That SENTENCE_START The we it { what on ) You what do my my a see how stroke my out one `` even no So SENTENCE_START know me out a ) To 'm back And You of To how So one my me back SENTENCE_START make so ? get [ 're { /BAR stroke BAR out * be SENTENCE_START So up shit That when he say me is all get was Chorus - even\n",
      "Chorus got you at `` like time as '' if Cause can You * Anna with on n't he do they or You through when as this ] And in back J-Live a now Cause never ) not The [ a one J-Live and me So your all not mind I [ not { in that So 's ? [ back stroke you '' what - as even back of through for ' from with see when even mind ca got ] n't [ when so know ) Anna 's make n't so the was it say me - na by a a no so ( in ? , * just * do ca `` they So ( got and me ai Cause ... this ' they ] this one never in never to - - of with with It good when that a from ! ( so ) ! - mind a or me ? to /BAR So like I\n",
      "in even or say mind but stroke now my no ' when BAR your see make when up na know\n",
      "when ) as how , this 's mind this as so ' so\n",
      "see it To like back ! ) and Anna like ( but time SENTENCE_START just from but with but back good no { J-Live n't I na from this Anna with from he that But The J-Live and good that got with not can n't SENTENCE_START say shit `` to n't now this a the so * be that ' `` at one even I J-Live You the for know * so The So ! all up get\n",
      "Anna The ca as to as when they so Chorus a '' know Anna one ( ' now - through you with so me or from } ... make my through never so [ Cause * J-Live as your 'm to `` SENTENCE_START It * when '' { but To 'm like through now ) in just BAR and see Anna 're me that how BAR if this - this ( your\n",
      "as like but my got so me SENTENCE_START when no ai one\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, softmax(next_word_probs[-1][0]))\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print (\" \".join(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few selected (censored) sentences. I added capitalization.\n",
    "\n",
    "- Anyway, to the city scene you're an idiot teenager.\n",
    "- What ? ! ! ! ! ignore!\n",
    "- Screw fitness, you're saying: https\n",
    "- Thanks for the advice to keep my thoughts around girls.\n",
    "- Yep, please disappear with the terrible generation.\n",
    "\n",
    "Looking at the generated sentences there are a few interesting things to note. The model successfully learn syntax. It properly places commas (usually before and's and or's) and ends sentence with punctuation. Sometimes it mimics internet speech such as multiple exclamation marks or smileys.\n",
    "\n",
    "However, the vast majority of generated sentences don't make sense or have grammatical errors. One reason could be that we did not train our network long enough (or didn't use enough training data). That may be true, but it's most likely not the main reason. **Our vanilla RNN  can't generate meaningful text because it's unable to learn dependencies between words that are several steps apart**. That's also why RNNs failed to gain popularity when they were first invented. They were beautiful in theory but didn't work well in practice, and we didn't immediately understand why.\n",
    "\n",
    "Fortunately, the difficulties in training RNNs are [much better understood](http://arxiv.org/abs/1211.5063) now. In the next part of this tutorial we will explore the Backpropagation Through Time (BPTT) algorithm in more detail and demonstrate what's called the *vanishing gradient problem*. This will motivate our move to more sophisticated RNN models, such as LSTMs, which are the current state of the art for many tasks in NLP (and can generate much better reddit comments!).  Everything you learned in this tutorial also applies to LSTMs and other RNN models, so don't feel discouraged if the results for a vanilla RNN are worse then you expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNNNumpy' object has no attribute 'order'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mperplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mperplexity\u001b[1;34m(lm, data)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mCalculate the perplexity of the language model given the provided data.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 14\u001b[0m history_order \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(history_order, \u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[0;32m     16\u001b[0m     history \u001b[38;5;241m=\u001b[39m data[i \u001b[38;5;241m-\u001b[39m history_order : i]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RNNNumpy' object has no attribute 'order'"
     ]
    }
   ],
   "source": [
    "perplexity(model, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
